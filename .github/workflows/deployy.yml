name: Deploy Strapi to AWS ECS Fargate Spot

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: us-east-2
  ECR_REPOSITORY: strapi-app
  ECS_SERVICE: strapi-service-vivek
  ECS_CLUSTER: strapi-cluster-vivek

jobs:
  deploy:
    name: Deploy to AWS
    runs-on: ubuntu-latest
    environment: production

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: Create ECR repository if it doesn't exist
      run: |
        aws ecr describe-repositories --repository-names $ECR_REPOSITORY --region $AWS_REGION || \
        aws ecr create-repository --repository-name $ECR_REPOSITORY --region $AWS_REGION

    - name: Build, tag, and push image to Amazon ECR
      id: build-image
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
      run: |
        # Build a docker container and push it to ECR
        echo "Building Docker image..."
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG . --no-cache
        
        echo "Pushing image to ECR..."
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        
        echo "Tagging as latest..."
        docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY:latest
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
        
        echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.5.0
        terraform_wrapper: false

    - name: Terraform Init
      run: terraform init
      working-directory: terraform7

    - name: Terraform Plan
      id: plan
      run: |
        terraform plan -no-color -input=false \
          -var="db_password=${{ secrets.DB_PASSWORD }}" \
          -var="ecr_image_url=${{ steps.build-image.outputs.image }}" \
          -var="alert_email=${{ secrets.ALERT_EMAIL }}" \
          -var="app_keys=${{ secrets.APP_KEYS }}" \
          -var="jwt_secret=${{ secrets.JWT_SECRET }}" \
          -var="api_token_salt=${{ secrets.API_TOKEN_SALT }}" \
          -var="admin_jwt_secret=${{ secrets.ADMIN_JWT_SECRET }}" \
          -var="transfer_token_salt=${{ secrets.TRANSFER_TOKEN_SALT }}" \
          -out=tfplan
      working-directory: terraform7

    - name: Terraform Apply
      id: terraform-apply
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        echo "TERRAFORM_APPLY_STARTED=true" >> $GITHUB_ENV
        terraform apply -auto-approve tfplan
        echo "TERRAFORM_APPLY_SUCCESS=true" >> $GITHUB_ENV
      working-directory: terraform7

    - name: Get outputs and create deployment summary
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      id: outputs
      run: |
        # Get Terraform outputs
        ALB_DNS=$(terraform output -raw alb_dns_name 2>/dev/null || echo "not-available")
        RDS_ENDPOINT=$(terraform output -raw rds_endpoint 2>/dev/null || echo "not-available")
        VPC_ID=$(terraform output -raw default_vpc_id 2>/dev/null || echo "not-available")
        SNS_TOPIC=$(terraform output -raw sns_topic_arn 2>/dev/null || echo "not-available")
        
        echo "alb_dns=$ALB_DNS" >> $GITHUB_OUTPUT
        echo "rds_endpoint=$RDS_ENDPOINT" >> $GITHUB_OUTPUT
        echo "vpc_id=$VPC_ID" >> $GITHUB_OUTPUT
        echo "sns_topic=$SNS_TOPIC" >> $GITHUB_OUTPUT
        
        # Create CloudWatch Dashboard URL
        DASHBOARD_URL="https://console.aws.amazon.com/cloudwatch/home?region=$AWS_REGION#dashboards:name=strapi-dashboard-vivek"
        echo "dashboard_url=$DASHBOARD_URL" >> $GITHUB_OUTPUT
        
        # Create ECS Service URL
        ECS_URL="https://console.aws.amazon.com/ecs/home?region=$AWS_REGION#/clusters/$ECS_CLUSTER/services/$ECS_SERVICE/details"
        echo "ecs_url=$ECS_URL" >> $GITHUB_OUTPUT
      working-directory: terraform7

    - name: Wait for ECS service to stabilize
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        echo "Waiting for ECS service to stabilize..."
        aws ecs wait services-stable --cluster $ECS_CLUSTER --services $ECS_SERVICE --region $AWS_REGION --max-attempts 10 --delay 30
        echo "ECS service is now stable!"

    - name: Update ECS service with new image
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        echo "Updating ECS service with new image..."
        aws ecs update-service --cluster $ECS_CLUSTER --service $ECS_SERVICE --force-new-deployment --region $AWS_REGION
        echo "ECS service update initiated!"

    - name: Verify deployment health
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      id: health-check
      run: |
        echo "Checking deployment health..."
        
        # Get ALB DNS
        ALB_DNS=$(terraform output -raw alb_dns_name 2>/dev/null || echo "")
        
        if [ -n "$ALB_DNS" ] && [ "$ALB_DNS" != "not-available" ]; then
          echo "Testing health endpoint..."
          
          # Wait for ALB to be ready
          sleep 60
          
          # Test health endpoint with retries
          for i in {1..5}; do
            if curl -f -s "http://$ALB_DNS/_health" || curl -f -s "http://$ALB_DNS/admin"; then
              echo "‚úÖ Health check passed!"
              echo "HEALTH_CHECK_PASSED=true" >> $GITHUB_ENV
              break
            else
              echo "‚ö†Ô∏è  Health check attempt $i failed, retrying in 30s..."
              sleep 30
            fi
          done
          
          if [ "$HEALTH_CHECK_PASSED" != "true" ]; then
            echo "‚ùå Health check failed after 5 attempts"
            echo "HEALTH_CHECK_FAILED=true" >> $GITHUB_ENV
            exit 1
          fi
        else
          echo "‚ö†Ô∏è  Could not get ALB DNS for health check"
        fi
      working-directory: terraform7

    - name: Create deployment summary
      if: github.ref == 'refs/heads/main' && github.event_name == 'push' && env.HEALTH_CHECK_PASSED == 'true'
      run: |
        cat << EOF >> $GITHUB_STEP_SUMMARY
        # üöÄ Strapi Deployment Summary
        
        ## ‚úÖ Deployment Status: SUCCESS
        
        ### üåê Application URLs
        - **Strapi Application**: http://${{ steps.outputs.outputs.alb_dns }}
        - **Admin Panel**: http://${{ steps.outputs.outputs.alb_dns }}/admin
        - **Health Check**: http://${{ steps.outputs.outputs.alb_dns }}/_health
        - **CloudWatch Dashboard**: ${{ steps.outputs.outputs.dashboard_url }}
        - **ECS Service**: ${{ steps.outputs.outputs.ecs_url }}
        
        ### üìä Infrastructure Details
        - **ALB DNS Name**: ${{ steps.outputs.outputs.alb_dns }}
        - **RDS Endpoint**: ${{ steps.outputs.outputs.rds_endpoint }}
        - **VPC ID**: ${{ steps.outputs.outputs.vpc_id }}
        - **Docker Image**: ${{ steps.build-image.outputs.image }}
        
        ### üîß Quick Links
        - [View Logs](https://console.aws.amazon.com/cloudwatch/home?region=${{ env.AWS_REGION }}#logsV2:log-groups/log-group/%2Fecs%2Fstrapi-task-vivek)
        - [ECS Cluster](https://console.aws.amazon.com/ecs/home?region=${{ env.AWS_REGION }}#/clusters/${{ env.ECS_CLUSTER }})
        - [RDS Instance](https://console.aws.amazon.com/rds/home?region=${{ env.AWS_REGION }}#database:id=strapi-postgres-db-vivek)
        
        ### ‚è∞ Deployment Info
        - **Commit**: ${{ github.sha }}
        - **Branch**: ${{ github.ref_name }}
        - **Deployed by**: ${{ github.actor }}
        - **Timestamp**: $(date -u)
        
        ---
        
        **Next Steps:**
        1. Your application is ready and healthy! üéâ
        2. Access your Strapi admin at: http://${{ steps.outputs.outputs.alb_dns }}/admin
        3. Monitor application health via the CloudWatch dashboard
        EOF

    - name: Comment PR with deployment info
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## üß™ Terraform Plan Results
            
            **Docker Image Built**: \`${{ steps.build-image.outputs.image }}\`
            
            > This is a preview of changes that will be applied when merged to main branch.
            
            ### Plan Summary:
            - Terraform configuration validated ‚úÖ
            - Docker image built and pushed to ECR ‚úÖ
            - Ready for deployment when merged to main`
          })

    # CLEANUP ON FAILURE STEPS
    - name: Terraform Destroy on Failure
      if: failure() && env.TERRAFORM_APPLY_STARTED == 'true'
      run: |
        echo "üßπ Deployment failed, initiating cleanup..."
        echo "This will destroy all created infrastructure to avoid charges."
        
        # Add a small delay to ensure resources are in a stable state
        sleep 30
        
        # Attempt to destroy with the same variables
        terraform destroy -auto-approve \
          -var="db_password=${{ secrets.DB_PASSWORD }}" \
          -var="ecr_image_url=${{ steps.build-image.outputs.image }}" \
          -var="alert_email=${{ secrets.ALERT_EMAIL }}" \
          -var="app_keys=${{ secrets.APP_KEYS }}" \
          -var="jwt_secret=${{ secrets.JWT_SECRET }}" \
          -var="api_token_salt=${{ secrets.API_TOKEN_SALT }}" \
          -var="admin_jwt_secret=${{ secrets.ADMIN_JWT_SECRET }}" \
          -var="transfer_token_salt=${{ secrets.TRANSFER_TOKEN_SALT }}" || \
        echo "‚ö†Ô∏è  Some resources may need manual cleanup"
        
        echo "üßπ Cleanup completed"
      working-directory: terraform7

    - name: Notify on failure with cleanup info
      if: failure()
      run: |
        cat << EOF >> $GITHUB_STEP_SUMMARY
        # ‚ùå Deployment Failed
        
        ## üßπ Cleanup Status
        $(if [ "$TERRAFORM_APPLY_STARTED" = "true" ]; then
          echo "‚úÖ **Terraform destroy executed** - Infrastructure cleanup attempted"
          echo ""
          echo "Most AWS resources should be cleaned up automatically to prevent charges."
          echo "Please verify in the AWS console that all resources are removed."
        else
          echo "‚ÑπÔ∏è  **No Terraform resources created** - No cleanup needed"
          echo ""
          echo "Failure occurred before infrastructure deployment."
        fi)
        
        ## Error Details
        The deployment to AWS failed. Infrastructure cleanup has been attempted.
        
        ### Common Issues:
        1. **Docker Build Failure**: 
           - Check if npm dependencies are correct
           - Verify Dockerfile syntax
           - Ensure all required files are present
        
        2. **AWS Credentials**: 
           - Ensure AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set correctly
           - Check IAM permissions for ECR, ECS, and RDS
        
        3. **Terraform Issues**:
           - Verify terraform files are in ./terraform7 directory
           - Check for syntax errors in .tf files
           - Ensure all required variables are provided
        
        4. **Secrets Configuration**:
           - Verify all required secrets are configured in GitHub repository settings
        
        5. **Health Check Failure**:
           - Application may not be responding correctly
           - Check ECS task logs for startup errors
           - Verify database connectivity
        
        ### Required Secrets:
        - AWS_ACCESS_KEY_ID
        - AWS_SECRET_ACCESS_KEY  
        - DB_PASSWORD
        - ALERT_EMAIL
        - APP_KEYS
        - JWT_SECRET
        - API_TOKEN_SALT
        - ADMIN_JWT_SECRET
        - TRANSFER_TOKEN_SALT
        
        ### Debug Steps:
        1. Check the logs above for specific error messages
        2. Verify your Dockerfile builds locally: \`docker build -t test .\`
        3. Test Terraform locally: \`terraform plan\`
        4. If resources weren't cleaned up automatically, run: \`terraform destroy\`
        
        ### Manual Cleanup (if needed):
        If automatic cleanup failed, you may need to manually remove:
        - ECS Service and Cluster
        - RDS Database Instance
        - Load Balancer and Target Groups
        - Security Groups
        - CloudWatch Log Groups
        - IAM Roles
        EOF

  # Optional: Manual cleanup job that can be triggered
  cleanup:
    name: Manual Cleanup
    runs-on: ubuntu-latest
    environment: production
    if: github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.5.0
        terraform_wrapper: false

    - name: Terraform Init
      run: terraform init
      working-directory: terraform7

    - name: Manual Terraform Destroy
      run: |
        echo "üßπ Initiating manual cleanup of all infrastructure..."
        terraform destroy -auto-approve \
          -var="db_password=${{ secrets.DB_PASSWORD }}" \
          -var="ecr_image_url=dummy" \
          -var="alert_email=${{ secrets.ALERT_EMAIL }}" \
          -var="app_keys=${{ secrets.APP_KEYS }}" \
          -var="jwt_secret=${{ secrets.JWT_SECRET }}" \
          -var="api_token_salt=${{ secrets.API_TOKEN_SALT }}" \
          -var="admin_jwt_secret=${{ secrets.ADMIN_JWT_SECRET }}" \
          -var="transfer_token_salt=${{ secrets.TRANSFER_TOKEN_SALT }}"
        echo "‚úÖ Manual cleanup completed"
      working-directory: terraform7